{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          workclass  fnlwgt    education  education-num  \\\n",
       "0       39          State-gov   77516    Bachelors             13   \n",
       "1       50   Self-emp-not-inc   83311    Bachelors             13   \n",
       "2       38            Private  215646      HS-grad              9   \n",
       "3       53            Private  234721         11th              7   \n",
       "4       28            Private  338409    Bachelors             13   \n",
       "...    ...                ...     ...          ...            ...   \n",
       "32556   27            Private  257302   Assoc-acdm             12   \n",
       "32557   40            Private  154374      HS-grad              9   \n",
       "32558   58            Private  151910      HS-grad              9   \n",
       "32559   22            Private  201490      HS-grad              9   \n",
       "32560   52       Self-emp-inc  287927      HS-grad              9   \n",
       "\n",
       "            marital-status          occupation    relationship    race  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   White   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "...                    ...                 ...             ...     ...   \n",
       "32556   Married-civ-spouse        Tech-support            Wife   White   \n",
       "32557   Married-civ-spouse   Machine-op-inspct         Husband   White   \n",
       "32558              Widowed        Adm-clerical       Unmarried   White   \n",
       "32559        Never-married        Adm-clerical       Own-child   White   \n",
       "32560   Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "\n",
       "           sex  capital-gain  capital-loss  hours-per-week  native-country  \\\n",
       "0         Male          2174             0              40   United-States   \n",
       "1         Male             0             0              13   United-States   \n",
       "2         Male             0             0              40   United-States   \n",
       "3         Male             0             0              40   United-States   \n",
       "4       Female             0             0              40            Cuba   \n",
       "...        ...           ...           ...             ...             ...   \n",
       "32556   Female             0             0              38   United-States   \n",
       "32557     Male             0             0              40   United-States   \n",
       "32558   Female             0             0              40   United-States   \n",
       "32559     Male             0             0              20   United-States   \n",
       "32560   Female         15024             0              40   United-States   \n",
       "\n",
       "        class  \n",
       "0       <=50K  \n",
       "1       <=50K  \n",
       "2       <=50K  \n",
       "3       <=50K  \n",
       "4       <=50K  \n",
       "...       ...  \n",
       "32556   <=50K  \n",
       "32557    >50K  \n",
       "32558   <=50K  \n",
       "32559   <=50K  \n",
       "32560    >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Census = pd.read_csv('adult.data', header=None, names=[\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "    'hours-per-week', 'native-country', 'class'\n",
    "])\n",
    "Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census['class'] = Census['class'].str.strip()\n",
    "Census['class'] = Census['class'].map({'>50K': 1, '<=50K': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          workclass  education-num       marital-status  \\\n",
       "0       39          State-gov             13        Never-married   \n",
       "1       50   Self-emp-not-inc             13   Married-civ-spouse   \n",
       "2       38            Private              9             Divorced   \n",
       "3       53            Private              7   Married-civ-spouse   \n",
       "4       28            Private             13   Married-civ-spouse   \n",
       "...    ...                ...            ...                  ...   \n",
       "32556   27            Private             12   Married-civ-spouse   \n",
       "32557   40            Private              9   Married-civ-spouse   \n",
       "32558   58            Private              9              Widowed   \n",
       "32559   22            Private              9        Never-married   \n",
       "32560   52       Self-emp-inc              9   Married-civ-spouse   \n",
       "\n",
       "               occupation    relationship      sex  capital-gain  \\\n",
       "0            Adm-clerical   Not-in-family     Male          2174   \n",
       "1         Exec-managerial         Husband     Male             0   \n",
       "2       Handlers-cleaners   Not-in-family     Male             0   \n",
       "3       Handlers-cleaners         Husband     Male             0   \n",
       "4          Prof-specialty            Wife   Female             0   \n",
       "...                   ...             ...      ...           ...   \n",
       "32556        Tech-support            Wife   Female             0   \n",
       "32557   Machine-op-inspct         Husband     Male             0   \n",
       "32558        Adm-clerical       Unmarried   Female             0   \n",
       "32559        Adm-clerical       Own-child     Male             0   \n",
       "32560     Exec-managerial            Wife   Female         15024   \n",
       "\n",
       "       capital-loss  hours-per-week  class  \n",
       "0                 0              40      0  \n",
       "1                 0              13      0  \n",
       "2                 0              40      0  \n",
       "3                 0              40      0  \n",
       "4                 0              40      0  \n",
       "...             ...             ...    ...  \n",
       "32556             0              38      0  \n",
       "32557             0              40      1  \n",
       "32558             0              40      0  \n",
       "32559             0              20      0  \n",
       "32560             0              40      1  \n",
       "\n",
       "[32561 rows x 11 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_columns = [\n",
    "    'age', \n",
    "    'workclass', \n",
    "    'education-num', \n",
    "    'marital-status', \n",
    "    'occupation', \n",
    "    'relationship', \n",
    "    'sex', \n",
    "    'capital-gain', \n",
    "    'capital-loss', \n",
    "    'hours-per-week', \n",
    "    'class'\n",
    "]\n",
    "Census = Census[relevant_columns]\n",
    "Census\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['workclass', 'marital-status', 'occupation', 'relationship', 'sex']\n",
    "Census = pd.get_dummies(Census, columns=categorical_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numerical_cols = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = StandardScaler()\n",
    "Census[numerical_cols] = scaler.fit_transform(Census[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_cols = Census.select_dtypes(include='bool').columns\n",
    "\n",
    "# Convert Boolean columns to integers (1 for True, 0 for False)\n",
    "Census[boolean_cols] = Census[boolean_cols].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'n_estimators': 100, 'max_features': 'log2', 'criterion': 'gini'}\n",
      "Test Set Accuracy: 83.60%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4942\n",
      "           1       0.65      0.68      0.67      1571\n",
      "\n",
      "    accuracy                           0.84      6513\n",
      "   macro avg       0.78      0.78      0.78      6513\n",
      "weighted avg       0.84      0.84      0.84      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Features and target\n",
    "X = Census.drop(\"class\", axis=1)  # Replace with your dataset\n",
    "y = Census[\"class\"]               # Replace with your dataset\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Number of combinations to try\n",
    "    scoring='accuracy',\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate on test set using the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.80%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4918\n",
      "           1       0.66      0.70      0.68      1595\n",
      "\n",
      "    accuracy                           0.84      6513\n",
      "   macro avg       0.78      0.79      0.79      6513\n",
      "weighted avg       0.84      0.84      0.84      6513\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      5026\n",
      "           1       0.64      0.69      0.66      1487\n",
      "\n",
      "    accuracy                           0.84      6513\n",
      "   macro avg       0.77      0.79      0.78      6513\n",
      "weighted avg       0.85      0.84      0.84      6513\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      4928\n",
      "           1       0.63      0.68      0.66      1585\n",
      "\n",
      "    accuracy                           0.83      6513\n",
      "   macro avg       0.76      0.78      0.77      6513\n",
      "weighted avg       0.83      0.83      0.83      6513\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.838016             0.659953          0.698433            0.678648\n",
      "1      2  0.839552             0.636252          0.694015            0.663879\n",
      "2      3  0.826808             0.634649          0.679495            0.656307\n",
      "Average Accuracy across the 3 trials: 83.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Train-test split with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=i)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest Classifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', max_features = 'log2', criterion = 'gini', random_state=i)\n",
    "    classifier.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.76%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     12323\n",
      "           1       0.66      0.68      0.67      3958\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.78      0.78      0.78     16281\n",
      "weighted avg       0.84      0.84      0.84     16281\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.62%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89     12476\n",
      "           1       0.64      0.69      0.66      3805\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.77      0.78      0.78     16281\n",
      "weighted avg       0.84      0.84      0.84     16281\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.38%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89     12345\n",
      "           1       0.65      0.68      0.66      3936\n",
      "\n",
      "    accuracy                           0.83     16281\n",
      "   macro avg       0.77      0.78      0.78     16281\n",
      "weighted avg       0.84      0.83      0.83     16281\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.837602             0.660872          0.681910            0.671226\n",
      "1      2  0.836189             0.639256          0.686465            0.662020\n",
      "2      3  0.833794             0.650073          0.676829            0.663181\n",
      "Average Accuracy across the 3 trials: 83.59%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Train-test split with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=i)\n",
    "    \n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=i)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest Classifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', max_features = 'log2', criterion = 'gini', random_state=i)\n",
    "    classifier.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.74%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     19729\n",
      "           1       0.66      0.68      0.67      6320\n",
      "\n",
      "    accuracy                           0.84     26049\n",
      "   macro avg       0.78      0.78      0.78     26049\n",
      "weighted avg       0.84      0.84      0.84     26049\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.42%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89     19845\n",
      "           1       0.64      0.69      0.66      6204\n",
      "\n",
      "    accuracy                           0.83     26049\n",
      "   macro avg       0.77      0.78      0.78     26049\n",
      "weighted avg       0.84      0.83      0.84     26049\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.15%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89     19752\n",
      "           1       0.65      0.66      0.66      6297\n",
      "\n",
      "    accuracy                           0.83     26049\n",
      "   macro avg       0.77      0.77      0.77     26049\n",
      "weighted avg       0.83      0.83      0.83     26049\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.837422             0.661253          0.676424            0.668752\n",
      "1      2  0.834235             0.641422          0.689394            0.664543\n",
      "2      3  0.831548             0.647916          0.663967            0.655843\n",
      "Average Accuracy across the 3 trials: 83.44%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Train-test split with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.8, random_state=i)\n",
    "    \n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=i)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest Classifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', max_features = 'log2', criterion = 'gini', random_state=i)\n",
    "    classifier.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 5 is smaller than n_iter=20. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Parameters: {'C': 10}\n",
      "Accuracy: 81.31%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87      4942\n",
      "           1       0.58      0.85      0.69      1571\n",
      "\n",
      "    accuracy                           0.81      6513\n",
      "   macro avg       0.76      0.82      0.78      6513\n",
      "weighted avg       0.85      0.81      0.82      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "}\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the SVM model\n",
    "svm_model = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svm_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 78.86%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.77      0.85      4918\n",
      "           1       0.54      0.85      0.66      1595\n",
      "\n",
      "    accuracy                           0.79      6513\n",
      "   macro avg       0.74      0.81      0.75      6513\n",
      "weighted avg       0.84      0.79      0.80      6513\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n",
      "Accuracy: 78.73%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85      5026\n",
      "           1       0.52      0.86      0.65      1487\n",
      "\n",
      "    accuracy                           0.79      6513\n",
      "   macro avg       0.73      0.81      0.75      6513\n",
      "weighted avg       0.85      0.79      0.80      6513\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 79.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85      4928\n",
      "           1       0.55      0.85      0.67      1585\n",
      "\n",
      "    accuracy                           0.80      6513\n",
      "   macro avg       0.75      0.81      0.76      6513\n",
      "weighted avg       0.85      0.80      0.81      6513\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.788577             0.543496          0.853918            0.664228\n",
      "1      2  0.787348             0.520799          0.858776            0.648388\n",
      "2      3  0.795179             0.551667          0.845426            0.667663\n",
      "Average Accuracy across the 3 trials: 79.04%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Perform train-test split with different random seeds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Train SVM classifier\n",
    "    classifier = SVC(kernel='linear', class_weight='balanced', C = 10, random_state=i)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 79.76%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.85     12323\n",
      "           1       0.55      0.87      0.68      3958\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.75      0.82      0.76     16281\n",
      "weighted avg       0.85      0.80      0.81     16281\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n",
      "Accuracy: 79.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.85     12476\n",
      "           1       0.54      0.85      0.66      3805\n",
      "\n",
      "    accuracy                           0.79     16281\n",
      "   macro avg       0.74      0.81      0.76     16281\n",
      "weighted avg       0.85      0.79      0.81     16281\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 79.63%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85     12345\n",
      "           1       0.55      0.85      0.67      3936\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.75      0.82      0.76     16281\n",
      "weighted avg       0.85      0.80      0.81     16281\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.797555             0.553387          0.866852            0.675527\n",
      "1      2  0.793563             0.536670          0.853876            0.659093\n",
      "2      3  0.796266             0.550646          0.854929            0.669852\n",
      "Average Accuracy across the 3 trials: 79.58%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Perform train-test split with different random seeds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=i)\n",
    "    \n",
    "    # Train SVM classifier\n",
    "    classifier = SVC(kernel='linear', class_weight='balanced', C = 10, random_state=i)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 80.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.86     19729\n",
      "           1       0.56      0.86      0.68      6320\n",
      "\n",
      "    accuracy                           0.80     26049\n",
      "   macro avg       0.75      0.82      0.77     26049\n",
      "weighted avg       0.85      0.80      0.81     26049\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n",
      "Accuracy: 79.83%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.86     19845\n",
      "           1       0.55      0.85      0.67      6204\n",
      "\n",
      "    accuracy                           0.80     26049\n",
      "   macro avg       0.75      0.82      0.76     26049\n",
      "weighted avg       0.85      0.80      0.81     26049\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 80.10%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.86     19752\n",
      "           1       0.56      0.86      0.68      6297\n",
      "\n",
      "    accuracy                           0.80     26049\n",
      "   macro avg       0.75      0.82      0.77     26049\n",
      "weighted avg       0.85      0.80      0.81     26049\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy  Precision (Class 1)  Recall (Class 1)  F1-Score (Class 1)\n",
      "0      1  0.799954             0.557053          0.856646            0.675104\n",
      "1      2  0.798265             0.549422          0.850258            0.667510\n",
      "2      3  0.801029             0.557601          0.856281            0.675393\n",
      "Average Accuracy across the 3 trials: 79.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# To store results for each test\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Perform train-test split with different random seeds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=i)\n",
    "    \n",
    "    # Train SVM classifier\n",
    "    classifier = SVC(kernel='linear', class_weight='balanced',C =10, random_state=i)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print results for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display overall results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "print(f\"Average Accuracy across the 3 trials: {average_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 16, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 16, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 16, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 32, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 32, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 32, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 64, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 64, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 32, 'hidden_size_2': 64, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 16, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 16, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 16, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 32, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 32, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 32, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 64, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 64, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 64, 'hidden_size_2': 64, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 16, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 16, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 16, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 32, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 32, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 32, 'lr': 0.1}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 64, 'lr': 0.001}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 64, 'lr': 0.01}\n",
      "\n",
      "Testing Parameters: {'hidden_size_1': 128, 'hidden_size_2': 64, 'lr': 0.1}\n",
      "\n",
      "Best Parameters and Results:\n",
      "Params      {'hidden_size_1': 128, 'hidden_size_2': 64, 'l...\n",
      "Accuracy                                             0.864579\n",
      "Name: 25, dtype: object\n",
      "\n",
      "Best Parameters:\n",
      "Hidden Layer 1 Size: 128\n",
      "Hidden Layer 2 Size: 64\n",
      "Learning Rate: 0.01\n",
      "\n",
      "Best Accuracy: 86.46%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"lr\": [0.001, 0.01, 0.1],                # Learning rates to tune\n",
    "    \"hidden_size_1\": [32, 64, 128],          # Hidden layer 1 sizes to tune\n",
    "    \"hidden_size_2\": [16, 32, 64],           # Hidden layer 2 sizes to tune\n",
    "}\n",
    "\n",
    "# Convert the grid to a list of parameter combinations\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "# To store results\n",
    "tuning_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for params in grid:\n",
    "    print(f\"\\nTesting Parameters: {params}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Define model with hyperparameters\n",
    "    class ANN(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(ANN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, params[\"hidden_size_1\"])\n",
    "            self.fc2 = nn.Linear(params[\"hidden_size_1\"], params[\"hidden_size_2\"])\n",
    "            self.fc3 = nn.Linear(params[\"hidden_size_2\"], 2)  # 2 output classes\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    model = ANN(input_size=X_train.shape[1])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 50  # Fixed number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).argmax(dim=1)\n",
    "        accuracy = (y_pred == y_test).float().mean().item()\n",
    "\n",
    "    # Append results\n",
    "    tuning_results.append({\n",
    "        \"Params\": params,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and display the best parameters\n",
    "results_df = pd.DataFrame(tuning_results)\n",
    "best_result = results_df.loc[results_df[\"Accuracy\"].idxmax()]\n",
    "print(\"\\nBest Parameters and Results:\")\n",
    "print(best_result)\n",
    "\n",
    "best_params = best_result[\"Params\"]\n",
    "best_accuracy = best_result[\"Accuracy\"]\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"Hidden Layer 1 Size: {best_params['hidden_size_1']}\")\n",
    "print(f\"Hidden Layer 2 Size: {best_params['hidden_size_2']}\")\n",
    "print(f\"Learning Rate: {best_params['lr']}\")\n",
    "print(f\"\\nBest Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Epoch [10/100], Loss: 0.4118\n",
      "Epoch [20/100], Loss: 0.3511\n",
      "Epoch [30/100], Loss: 0.3376\n",
      "Epoch [40/100], Loss: 0.3171\n",
      "Epoch [50/100], Loss: 0.3082\n",
      "Epoch [60/100], Loss: 0.3034\n",
      "Epoch [70/100], Loss: 0.2995\n",
      "Epoch [80/100], Loss: 0.2974\n",
      "Epoch [90/100], Loss: 0.2951\n",
      "Epoch [100/100], Loss: 0.2932\n",
      "Accuracy: 0.8440\n",
      "\n",
      "--- Trial 2 ---\n",
      "Epoch [10/100], Loss: 0.4440\n",
      "Epoch [20/100], Loss: 0.4176\n",
      "Epoch [30/100], Loss: 0.3627\n",
      "Epoch [40/100], Loss: 0.3345\n",
      "Epoch [50/100], Loss: 0.3221\n",
      "Epoch [60/100], Loss: 0.3142\n",
      "Epoch [70/100], Loss: 0.3073\n",
      "Epoch [80/100], Loss: 0.3029\n",
      "Epoch [90/100], Loss: 0.3016\n",
      "Epoch [100/100], Loss: 0.2988\n",
      "Accuracy: 0.8558\n",
      "\n",
      "--- Trial 3 ---\n",
      "Epoch [10/100], Loss: 0.4408\n",
      "Epoch [20/100], Loss: 0.4211\n",
      "Epoch [30/100], Loss: 0.3764\n",
      "Epoch [40/100], Loss: 0.3505\n",
      "Epoch [50/100], Loss: 0.3336\n",
      "Epoch [60/100], Loss: 0.3230\n",
      "Epoch [70/100], Loss: 0.3141\n",
      "Epoch [80/100], Loss: 0.3078\n",
      "Epoch [90/100], Loss: 0.3029\n",
      "Epoch [100/100], Loss: 0.2996\n",
      "Accuracy: 0.8412\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy\n",
      "0      1  0.844004\n",
      "1      2  0.855827\n",
      "2      3  0.841241\n",
      "Average Accuracy: 84.70%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ANN\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Split data with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=i\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANN(input_size=X_train.shape[1])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).argmax(dim=1)\n",
    "        accuracy = (y_pred == y_test).float().mean().item()\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append results\n",
    "    results.append({\"Trial\": i + 1, \"Accuracy\": accuracy})\n",
    "\n",
    "# Summary of results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "print(f\"Average Accuracy: {results_df['Accuracy'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Epoch [10/100], Loss: 0.4434\n",
      "Epoch [20/100], Loss: 0.4066\n",
      "Epoch [30/100], Loss: 0.3590\n",
      "Epoch [40/100], Loss: 0.3408\n",
      "Epoch [50/100], Loss: 0.3261\n",
      "Epoch [60/100], Loss: 0.3156\n",
      "Epoch [70/100], Loss: 0.3075\n",
      "Epoch [80/100], Loss: 0.3011\n",
      "Epoch [90/100], Loss: 0.2949\n",
      "Epoch [100/100], Loss: 0.2874\n",
      "Accuracy: 0.8476\n",
      "\n",
      "--- Trial 2 ---\n",
      "Epoch [10/100], Loss: 0.5551\n",
      "Epoch [20/100], Loss: 0.3614\n",
      "Epoch [30/100], Loss: 0.3341\n",
      "Epoch [40/100], Loss: 0.3225\n",
      "Epoch [50/100], Loss: 0.3145\n",
      "Epoch [60/100], Loss: 0.3082\n",
      "Epoch [70/100], Loss: 0.3033\n",
      "Epoch [80/100], Loss: 0.2986\n",
      "Epoch [90/100], Loss: 0.2939\n",
      "Epoch [100/100], Loss: 0.2900\n",
      "Accuracy: 0.8542\n",
      "\n",
      "--- Trial 3 ---\n",
      "Epoch [10/100], Loss: 0.5512\n",
      "Epoch [20/100], Loss: 0.4488\n",
      "Epoch [30/100], Loss: 0.4004\n",
      "Epoch [40/100], Loss: 0.3624\n",
      "Epoch [50/100], Loss: 0.3423\n",
      "Epoch [60/100], Loss: 0.3296\n",
      "Epoch [70/100], Loss: 0.3206\n",
      "Epoch [80/100], Loss: 0.3130\n",
      "Epoch [90/100], Loss: 0.3074\n",
      "Epoch [100/100], Loss: 0.3030\n",
      "Accuracy: 0.8473\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy\n",
      "0      1  0.847614\n",
      "1      2  0.854186\n",
      "2      3  0.847307\n",
      "Average Accuracy: 84.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ANN\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Split data with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=i\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANN(input_size=X_train.shape[1])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).argmax(dim=1)\n",
    "        accuracy = (y_pred == y_test).float().mean().item()\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append results\n",
    "    results.append({\"Trial\": i + 1, \"Accuracy\": accuracy})\n",
    "\n",
    "# Summary of results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "print(f\"Average Accuracy: {results_df['Accuracy'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Epoch [10/100], Loss: 0.6022\n",
      "Epoch [20/100], Loss: 0.3669\n",
      "Epoch [30/100], Loss: 0.3059\n",
      "Epoch [40/100], Loss: 0.2832\n",
      "Epoch [50/100], Loss: 0.2679\n",
      "Epoch [60/100], Loss: 0.2545\n",
      "Epoch [70/100], Loss: 0.2446\n",
      "Epoch [80/100], Loss: 0.2438\n",
      "Epoch [90/100], Loss: 0.2315\n",
      "Epoch [100/100], Loss: 0.2407\n",
      "Accuracy: 0.8388\n",
      "\n",
      "--- Trial 2 ---\n",
      "Epoch [10/100], Loss: 0.4330\n",
      "Epoch [20/100], Loss: 0.4407\n",
      "Epoch [30/100], Loss: 0.3844\n",
      "Epoch [40/100], Loss: 0.3616\n",
      "Epoch [50/100], Loss: 0.3419\n",
      "Epoch [60/100], Loss: 0.3288\n",
      "Epoch [70/100], Loss: 0.3185\n",
      "Epoch [80/100], Loss: 0.3086\n",
      "Epoch [90/100], Loss: 0.2977\n",
      "Epoch [100/100], Loss: 0.2959\n",
      "Accuracy: 0.8453\n",
      "\n",
      "--- Trial 3 ---\n",
      "Epoch [10/100], Loss: 0.5560\n",
      "Epoch [20/100], Loss: 0.4584\n",
      "Epoch [30/100], Loss: 0.3913\n",
      "Epoch [40/100], Loss: 0.3521\n",
      "Epoch [50/100], Loss: 0.3324\n",
      "Epoch [60/100], Loss: 0.3150\n",
      "Epoch [70/100], Loss: 0.3031\n",
      "Epoch [80/100], Loss: 0.2928\n",
      "Epoch [90/100], Loss: 0.2834\n",
      "Epoch [100/100], Loss: 0.2748\n",
      "Accuracy: 0.8411\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy\n",
      "0      1  0.838804\n",
      "1      2  0.845292\n",
      "2      3  0.841107\n",
      "Average Accuracy: 84.17%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ANN\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 random tests\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "    \n",
    "    # Split data with a different random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.8, random_state=i\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANN(input_size=X_train.shape[1])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).argmax(dim=1)\n",
    "        accuracy = (y_pred == y_test).float().mean().item()\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append results\n",
    "    results.append({\"Trial\": i + 1, \"Accuracy\": accuracy})\n",
    "\n",
    "# Summary of results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "print(f\"Average Accuracy: {results_df['Accuracy'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam Tuning Using Normal Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "best_params = None\n",
    "best_auc = 0\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) * 0.8\n",
    "for n_estimators in [100, 200, 300]:\n",
    "    for max_depth in [3, 5, 7]:\n",
    "        for learning_rate in [0.01, 0.05, 0.1]:\n",
    "            xgb = XGBClassifier(\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                random_state=42,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            xgb.fit(X_train, y_train)\n",
    "            y_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 84.03%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89      4918\n",
      "           1       0.63      0.84      0.72      1595\n",
      "\n",
      "    accuracy                           0.84      6513\n",
      "   macro avg       0.79      0.84      0.80      6513\n",
      "weighted avg       0.87      0.84      0.85      6513\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.37%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89      5026\n",
      "           1       0.60      0.84      0.70      1487\n",
      "\n",
      "    accuracy                           0.83      6513\n",
      "   macro avg       0.77      0.84      0.79      6513\n",
      "weighted avg       0.87      0.83      0.84      6513\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 83.92%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89      4928\n",
      "           1       0.63      0.82      0.71      1585\n",
      "\n",
      "    accuracy                           0.84      6513\n",
      "   macro avg       0.78      0.83      0.80      6513\n",
      "weighted avg       0.86      0.84      0.85      6513\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy   ROC-AUC  Precision (Class 1)  Recall (Class 1)  \\\n",
      "0      1  0.840319  0.924611             0.630711          0.839498   \n",
      "1      2  0.833717  0.922657             0.596743          0.837929   \n",
      "2      3  0.839245  0.921447             0.631092          0.817035   \n",
      "\n",
      "   F1-Score (Class 1)  \n",
      "0            0.720280  \n",
      "1            0.697063  \n",
      "2            0.712125  \n",
      "\n",
      "Average Accuracy across trials: 83.78%\n",
      "Average ROC-AUC across trials: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 trials\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "\n",
    "    # Split the data with a different random seed each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=i\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight to handle class imbalance\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) * 0.8\n",
    "\n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_estimators=200,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=i,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    y_proba = xgb.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "    # Print metrics for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Summarize results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "\n",
    "# Calculate and print the average metrics across trials\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "average_roc_auc = results_df[\"ROC-AUC\"].mean()\n",
    "print(f\"\\nAverage Accuracy across trials: {average_accuracy * 100:.2f}%\")\n",
    "print(f\"Average ROC-AUC across trials: {average_roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 84.56%\n",
      "ROC-AUC Score: 0.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89     12323\n",
      "           1       0.64      0.83      0.72      3958\n",
      "\n",
      "    accuracy                           0.85     16281\n",
      "   macro avg       0.79      0.84      0.81     16281\n",
      "weighted avg       0.87      0.85      0.85     16281\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.99%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89     12476\n",
      "           1       0.62      0.83      0.71      3805\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.78      0.84      0.80     16281\n",
      "weighted avg       0.87      0.84      0.85     16281\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 84.19%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89     12345\n",
      "           1       0.64      0.81      0.71      3936\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.78      0.83      0.80     16281\n",
      "weighted avg       0.86      0.84      0.85     16281\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy   ROC-AUC  Precision (Class 1)  Recall (Class 1)  \\\n",
      "0      1  0.845587  0.927194             0.640194          0.832996   \n",
      "1      2  0.839936  0.924317             0.617480          0.828121   \n",
      "2      3  0.841902  0.922586             0.635603          0.810976   \n",
      "\n",
      "   F1-Score (Class 1)  \n",
      "0            0.723979  \n",
      "1            0.707454  \n",
      "2            0.712659  \n",
      "\n",
      "Average Accuracy across trials: 84.25%\n",
      "Average ROC-AUC across trials: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 trials\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "\n",
    "    # Split the data with a different random seed each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.5, random_state=i\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight to handle class imbalance\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) * 0.8\n",
    "\n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_estimators=200,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=i,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    y_proba = xgb.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "    # Print metrics for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Summarize results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "\n",
    "# Calculate and print the average metrics across trials\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "average_roc_auc = results_df[\"ROC-AUC\"].mean()\n",
    "print(f\"\\nAverage Accuracy across trials: {average_accuracy * 100:.2f}%\")\n",
    "print(f\"Average ROC-AUC across trials: {average_roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Accuracy: 84.15%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89     19729\n",
      "           1       0.63      0.82      0.72      6320\n",
      "\n",
      "    accuracy                           0.84     26049\n",
      "   macro avg       0.79      0.84      0.80     26049\n",
      "weighted avg       0.86      0.84      0.85     26049\n",
      "\n",
      "\n",
      "--- Trial 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.15%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89     19845\n",
      "           1       0.63      0.81      0.71      6204\n",
      "\n",
      "    accuracy                           0.84     26049\n",
      "   macro avg       0.78      0.83      0.80     26049\n",
      "weighted avg       0.86      0.84      0.85     26049\n",
      "\n",
      "\n",
      "--- Trial 3 ---\n",
      "Accuracy: 84.20%\n",
      "ROC-AUC Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89     19752\n",
      "           1       0.63      0.82      0.72      6297\n",
      "\n",
      "    accuracy                           0.84     26049\n",
      "   macro avg       0.79      0.84      0.80     26049\n",
      "weighted avg       0.86      0.84      0.85     26049\n",
      "\n",
      "\n",
      "Summary of Results Across Trials:\n",
      "   Trial  Accuracy   ROC-AUC  Precision (Class 1)  Recall (Class 1)  \\\n",
      "0      1  0.841491  0.923277             0.633386          0.823101   \n",
      "1      2  0.841453  0.920960             0.629270          0.813669   \n",
      "2      3  0.842028  0.922341             0.633537          0.821979   \n",
      "\n",
      "   F1-Score (Class 1)  \n",
      "0            0.715888  \n",
      "1            0.709686  \n",
      "2            0.715560  \n",
      "\n",
      "Average Accuracy across trials: 84.17%\n",
      "Average ROC-AUC across trials: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:17:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Perform 3 trials\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Trial {i + 1} ---\")\n",
    "\n",
    "    # Split the data with a different random seed each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.8, random_state=i\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight to handle class imbalance\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) * 0.8\n",
    "\n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_estimators=200,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=i,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    y_proba = xgb.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Trial\": i + 1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision (Class 1)\": report[\"1\"][\"precision\"],\n",
    "        \"Recall (Class 1)\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score (Class 1)\": report[\"1\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "    # Print metrics for this trial\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Summarize results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Results Across Trials:\")\n",
    "print(results_df)\n",
    "\n",
    "# Calculate and print the average metrics across trials\n",
    "average_accuracy = results_df[\"Accuracy\"].mean()\n",
    "average_roc_auc = results_df[\"ROC-AUC\"].mean()\n",
    "print(f\"\\nAverage Accuracy across trials: {average_accuracy * 100:.2f}%\")\n",
    "print(f\"Average ROC-AUC across trials: {average_roc_auc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
